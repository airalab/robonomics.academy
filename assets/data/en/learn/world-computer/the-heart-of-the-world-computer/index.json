{"hash":"a71565a3f903c892e0bce6821a9959ab726c6e6c","data":{"course":{"id":"2d0f49f668a8ad8625905bc77ec2e732","title":"Part 2: The Heart of the World Computer","description":"What lies at the core of projects like Ethereum or Polkadot, or any other web3 project claiming the title of the world computer, and why does the comparison with the heart in the human body fit so well into the abstract architecture of the world computer?","content":"\nWhat lies at the core of projects like Ethereum or Polkadot, or any other web3 project claiming the title of the world computer, and why does the comparison with the heart in the human body fit so well into the abstract architecture of the world computer?\n\nLet's try to understand these questions in this part of the lecture, and to begin with, we'll have to break the chains of Bitcoin maximalists a bit. Most likely, you've already read one or several popular science articles dedicated to Bitcoin in your life, and the main thing that is practically noted everywhere is the three main advantages of Bitcoin as electronic cash:\n\n- Censorship resistance\n- Immutability of data stored in the Bitcoin blockchain\n- Transparency of transactions\n\nLet's quickly go through each of these properties, and most importantly, at the end, we'll need to step back a bit from them, as the world computer inherits them as some kind of successor to Bitcoin.\n\n**Firstly**, immutability of data. Of course, this property, from the perspective of electronic cash, has significant advantages and importance. After you send a transaction or receive bitcoins, after one or two blocks, you gradually begin to feel the immutability of data in the blockchain. In the first 15 minutes, you can observe, using a blockchain explorer, how the transaction should settle. You already see it in the network, but it is not finalized, meaning these bitcoins are not yet in your account. However, after several blocks, there is confidence that these funds will not disappear from your account. As time passes, the probability of values being somehow overwritten from your account becomes almost negligible, practically reducing to zero. This is our property of data immutability. How cool it is when you can share information with the recipient, send them a link to the blockchain explorer, and you don't need to obtain any paper or document from the bank stating that you sent a payment on a certain date and time - this is the second advantage of Bitcoin that is very helpful in practice and is probably the most pleasant perk when comparing a bank transfer and a Bitcoin transfer.\n\n**Secondly**, transparency of transactions. There is practically no fear when using Bitcoin that you will find yourself in a region of the world or connected to an internet provider through which you cannot perform operations with the Bitcoin network. There are practically no options other than locking you in a dark room without internet access so that you cannot use the Bitcoin network.\n\nThese three properties are, of course, very important. Now, in order to understand the question \"What is at the heart of the world computer,\" we will need to step back from them, abstract ourselves, and make a small leap of faith, a jump, approximately, 100 years back to the 1930s.\nIn 1936, Alan Turing made a proposal to the scientific community to solve the formalization problem and, in fact, a more qualitative description of such a concept as an algorithm. Interestingly, from Alan Turing's proposal, the architecture and development of all computer science in the future emerged, but he in no way pursued the idea of creating a personal computer, and he knew nothing about data centers or clouds. His task was to provide a way to define an algorithm in the context of the tasks faced by mathematicians. It's a fascinating picture over the span of a century that the achievement of computer science turns out to be a by-product of a rather mundane problem among mathematicians.\n\nLet's delve into what Alan Turing proposed, without delving too deeply into algorithm theory and the purpose for which he suggested it. Alan Turing proposed the Turing machine, which represents an infinite tape (we can call it memory cells to make it easier), traversed by a reading and writing head. This head, positioned over a certain cell, can read data, apply some simple operations to them, and write new values.\n\nToday, when you hear phrases like this, it might seem to you: \"Well, yes, it's a hard drive, a computer, or something like that.\" That's absolutely correct. This description gave rise to the first computer architecture. However, the main task of the Turing machine was to provide a means of representing a system or entity capable of performing any formalized computations. One can imagine a box or room, even filled with lamps, into which you insert your punch card, card, or transmit a Bluetooth signal, and the machine starts working, performing simple operations that ultimately solve your problem. Thus, the Turing machine is a universal computational mechanism that primarily solves the universal and essential task of providing a mechanism through which any simple computation or, more accurately, any formalized computation can be performed—computations that can be decomposed into the language of mathematics.\n\nFrom this approach to problem-solving and the theory of algorithms, the personal computer emerged. From this definition, in 2014, the concept of Ethereum as the first world computer essentially appeared. Let's briefly describe what it represents. We have a set of transactions or computation requests as input. These requests go to a computational machine or, if we talk about its theoretical essence, a \"state transition,\" the actual change of state. Remember, just a couple of minutes ago, I talked about the infinite tape and the reading head moving along it? So, the head traversing this tape effectively changes the state of each cell it works with. At the output, we get outcomes. We're not talking about the specific effect of writing to a cell, not discussing any additional storage operations, just that by inputting a set of data or reading them from the infinite tape, we can apply a set of formalized rules written on our moving head along the tape. After applying a set of operations to the data, we get output values that can be recorded in a cell or transmitted further along wires, and not only by air, by wires, but also possibly by other means in the future.\nIt is precisely the state transition function that lies at the heart of the world computer. It is from this definition by Alan Turing of the computational machine to refine the theory of the algorithm that the concept of an algorithm emerges, and, in principle, it doesn't matter whether it's a personal computer or a data center. A small addition, of course, after the theoretical solution was found to create a computer, the task ultimately focused on finding its hardware form. Therefore, it's not surprising that when we talk about the world computer today, we refer to a theory that is almost a century old. It took, you could say, a full 100 years to transition from an abstract description of a computer to its actual implementation in the form of servers, your pocket smartphone, or the laptop on your desk.\n\nIn essence, the task of the last 100 years, after finding some solution in the field of mathematics, was precisely to give it a physical form, to find the set of transistors that could be placed on a board, learn how to solder them all, reduce the processes of the computing processor, and so on. No wonder that the theory from 1936 finds application in 2014 for the ideas of the world computer. These 100 years were occupied, in general, in another area—the field of physically implementing this computer.\n\nWhen the planet became saturated, and we had personal computers, computational machines even inside smart devices, and when data centers started growing on the planet, the question shifted from the hardware solution to how the computational machine might look not at the physical or mathematically abstract level but at some non-physical, perhaps metaphysical, level relative to the entire planet. However, the foundation remains the same: the state transition function and nothing else.\n\nAs an addition to what has been said, so that we don't only dwell on the theory of 1936 and don't just break the shackles of Bitcoin maximalists, open the Ethereum white paper. There you will find the crucial phrase \"Turing complete machine\"—this is the main definition of Ethereum. A Turing complete machine means that Ethereum can handle any simple operations described in a formal language, operations that are possible. This is not some set of operations that Ethereum can provide as a calculator or a sophisticated calculator for scientists. Instead, it is an abstraction inside which it is possible to load any possible variations, manipulations with variables, constants, additions, calculations with any states, and so on. You won't find anything different from what Turing proposed in the 1930s in the Ethereum concept. You will find an engineering implementation of how to do it. If we move on and open the Polkadot wiki, it's a bit more challenging to find. For this, you should use the search, enter \"State transition,\" and in the search results, find several mentions that Polkadot guarantees nothing else but the state transition. Neither the storage of data in the Polkadot blockchain nor any additional services—only the purest change of state caused by incoming transactions and processed by Polkadot validators. Now, let's try to delve more into this.\n\nNow, let's add a bit to this linear diagram to move from the theory of the 1930s to today's realities, where we describe the abstract picture of the world computer. To do this, let's consider an example with Alice and Bob. Alice, being in the office, wants to start Bob's home vacuum robot for cleaning. If we look at today's concepts of how the link between Alice's mobile application and the robot vacuum at home is implemented, you will see roughly the following picture: Alice's mobile application generates a transaction in some cloud where calculations take place, and the output of these calculations is the output values that effectively turn into a command to start the vacuum robot. It would be useful for us, from the field of robotics and Robonomics as concepts in the world of web3, to understand that in the cloud, there is a digital twin of this robot, and its state is changed. We can, in general, not go that far and stop at the fact that Alice sends a transaction to the cloud, and the cloud, having performed all the necessary calculations and manipulations, generates a command to start Bob's vacuum robot.\n\nIn this scheme today, there are several main questions: if you were interacting with a physical computer in front of you or were in a room with the vacuum robot, you would approach, press a physical button, and set it in motion. What changes when instead of arrows, there is not a manual drive but a communication layer, the internet? A multitude of questions arises about how we can safely connect Alice and this cloud, how we can be sure that Alice has access to this cloud. The question of the communication network arises—how we can protect Alice from someone else addressing her vacuum robot, requesting, for example, to make a video of her entire apartment instead of cleaning, and a similar aspect arises: why would the vacuum robot listen to this cloud with such honor and integrity? Why would the robot fully trust this cloud?\n\nToday's approach with the architecture of cloud solutions that connect your mobile phone, or rather, the application on your mobile phone, and some technology on the other side, smart devices, is based on the significant achievements in building physical computers. Computers in data centers today are something extraordinary—the level of technical processes is simply amazing. However, from the perspective of communication technologies, when you already have some experience working with internet applications, it seems that somewhere there, at the level of a technical school or college, or maybe not right next to how developers, architects of Intel processors are solving their tasks now. Almost all questions about connecting Alice to Bob boil down solely to outputting a specific access certificate on a specific IP address from both sides, linking them together, and the cloud will own and do anything. The most important thing in this scheme is to do anything, meaning to perform state transitions or operations that occur without any guarantees that for Alice, for Bob, these will be executed according to the same logic. No one can say anything about how the cloud is arranged. It is a black box where computations are not formalized, and neither Alice nor Bob knows how the computation is performed.\n\nThe place where you must fully trust—relying on the reputation of the company that owns these data centers, and you must completely trust the network access providers who issue a certificate and verify the security of your connection. In fact, if we talk about the boom of internet applications, this is a huge problem. The problem is that there are actually some citadels located in specific jurisdictions that operate on a relatively simple technology stack to connect you as easily as possible to the cloud, which represents a black box. Dissatisfaction with this approach actually arouses interest in the world computer because it will arrange things a bit differently. And how? Let's try to supplement the scheme we drew with blue color right now.\n\nSo, to supplement our linear graph, our linear diagram from both sides, let's take a look at the discoveries that have significance in computer science and that are directly or indirectly related to achievements from the world of web3.\n\nLet's start with Leslie Lamport in 1976. Those who attended my presentations, lectures from 2015-2020, probably remember how often I liked to mention that before the invention of Bitcoin, problems related to creating a decentralized network were well described by Leslie Lamport in 1976 in the Byzantine Generals problem. The solution to the Byzantine Generals problem is at the core of Tendermint PBFT algorithms and all synchronous algorithms used from Telegram Open Network to Tendermint, to Cosmos, and other blockchain projects that, accordingly, followed the path of Byzantine Generals.\n\nThe second interesting achievement in internet technologies is torrent trackers. We don't have any specific, already erased, cloud or a black box that stores files. Still, users worldwide, by exchanging torrent files, can download exactly the file they were looking for, and this works without data substitution. No one uploads any viruses to you by replacing the file. There might sometimes be a virus embedded in the file, but the idea of receiving a link to download and actually downloading something other than what you were offered to download using torrent technology is impossible. Similar processes exist in the IPFS network, a hash-oriented storage - a way of connecting various participants with trust in the information you convey without using a black box, precisely.\n\nAnd of course, Bitcoin. Bitcoin, as a more collective example, I'm sure Satoshi Nakamoto was well aware of Leslie Lamport's solution to the Byzantine Generals problem and, of course, observed how the idea of torrent trackers was developing. If we don't emphasize the properties that the Bitcoin blockchain obtained, such as immutability, transparency of transactions, and, to some extent, censorship resistance, then Bitcoin is an internet service that performs state transition, some changes in state based on transactions without a central node. It is an example of a collective construction of a global network in which there is a constantly functioning state transition function that we can trust, and to ensure trust, neither jurisdictions nor specific IP addresses nor the most primitive technologies used and still used today in building cloud services are used. The collective image of Bitcoin allowed overlaying the general concept from the 1930s of a Turing-complete machine on the existence of a universal abstract function for everything.\n\nSo, what do we need to add to this scheme to envision a global computer? From the bottom, we provide consensus validators or, in general, validators. It can be said that the \"Data availability layer\" is probably a phrase many have heard around Ethereum this year, and it has become an advantage of Bitcoin as well. However, in the organization scheme of the global computer, this is one piece of the puzzle and, as I mentioned, it complements the main function that lies at the heart of the global computer—the function of universal state transition. Going back to the very beginning, the analogy of the heart in the human body is interesting here. It's not a thinking thing, indeed. Yes, it doesn't generate, you could say, the brain is much more important. Still, life is impossible without the heart. It simply pumps blood. Similarly, at the core of the global computer, all transactions are pumped through the state transition function, resulting in outputs. But to organize this in a distributed internet network without the need to trust some citadel, we need to supplement the picture with two components.\n\nThe first component is a set of computers or nodes that are ready to execute the state transition. When you send transactions, they don't just go here; they go to the validators. Validators perform computations, recalling what I've already mentioned in this ongoing conversation. They take your transaction, retrieve information from the blockchain about how to process that transaction, apply that processing, and then coordinate with other validators the fact that they correctly executed the state transition. The core of the global computer, in terms of protection against situations where Bob, the vacuum cleaner robot, receives a correct command from Alice in the office, is not based on trust but on cross-verification by a multitude of network participants based on available information from the blockchain. Not only from the blockchain, by the way. It's complex, and we won't delve into it right now, but essentially, a multitude of validators take turns watching and have incentives, some internal incentives within the protocol, to prevent the universal and capable-of-calculating-anything machine from executing this operation incorrectly. A validator effectively processes transactions that come into the global computer, and other validators help prevent situations where one of the validators performed an incorrect calculation. The better the consensus algorithms of the validators, the better protection we have for the state transition function or, in other words, the heart of our global computer.\n\nThe second part of this scheme is the data availability service—what we've always called the database in Bitcoin or Ethereum. In fact, we'll have to abandon that concept because there's a fundamental change in the architecture of all projects, and for those specifically targeting the global computer, this change is most crucial. For a simple present-day example: there are various implementations of Layer 2 networks on top of Ethereum—such as Arbitrum, Optimism, and others. If you start looking into their main differences and how they operate, you'll find that, in some cases, an L2 network in Ethereum sends a larger amount of data, solely from the first-layer blockchain, i.e., from the Ethereum blockchain. All the necessary puzzle pieces to confirm that the computation on the L2 layer was correct can be found in the first-layer Ethereum blockchain. On the other hand, other approaches suggest that beyond the first layer of Ethereum, something else is stored that needs to be found to prove the correctness of transactions. So, right now, before our eyes, there is again a question of improvement, but specifically of such an architecture where transactions go on the left, in the middle, we have the heart in the form of the state transition function, validators, and their consensus allows for the correct execution of this state transition. But there is also a question of data availability, which is necessary to ensure both cross-verification and, essentially, the existence of the service itself. Some approaches and patterns for creating L2 on top of Ethereum today ask the question: \"What if a certain L2 layer loses the data it doesn't store within the main Ethereum blockchain?\"\n\nLet's complement this picture with how Polkadot is structured. Polkadot has two consensus mechanisms: the \"babe\" consensus, responsible for the parachain-level consensus and is fast, and the \"grandpa\" consensus, which is slower and verifies everything afterward. So, if you delve into the wiki article titled \"The Path of a Block in the Polkadot Network,\" you will encounter interesting abbreviations. After achieving the \"babe\" consensus at the parachain level, the \"grandpa\" consensus introduces the concept of \"proof of validity and data availability.\" Going deeper, you'll find the term \"chunk\" of redundant pieces of information, inspired by CD RW technologies from the 90s and 2000s. This addresses the question of how to preserve information when absolute trust in a specific entity in the network is not feasible. The concept of \"chunk\" of redundant information is one of these patterns.\n\nSumming up, at the core lies an abstract function that enables any computation and was described by Alan Turing in the 1930s. The personal computer, essentially a side effect of a mathematical problem, emerged from Turing's work. The technologies first applied in Bitcoin, such as consensus that allows the network to exist without a specific data center or entity responsible for data correctness, form a functioning mechanism. It goes beyond providing a specific service for electronic cash transfers; it allows us to audit and control any computation in the network. Additionally, we face the challenge of ensuring data availability, as it's not the primary concern of the world computer. The world computer's task lies at its core, executing computation, managing state transitions, and performing calculations, while the data in this scheme serves as a puzzle piece that is more necessary to support the lower part. Thus, this overall scheme can be seen as an abstract and generalized illustration of the world computer's structure, where the state transition function is at its core.\n","fileInfo":{"path":"en/learn/world-computer/the-heart-of-the-world-computer.md","name":"the-heart-of-the-world-computer"},"defaultName":"World computer in your home","lastUpdate":null}},"context":{}}