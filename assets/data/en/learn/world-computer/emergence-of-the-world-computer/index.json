{"hash":"76c8f6c333fd43be28237b9e09f4e90eec13e11a","data":{"course":{"id":"e9f302c4ead6ee0a019317cf690f8904","title":"Part 3: Emergence of the World Computer","description":"In the third part titled \"Emergence of the World Computer,\" we will attempt, layer by layer, to recreate the engineering implementation of the world computer using examples from Ethereum and Polkadot, as before.","content":"\nIn the third part titled \"Emergence of the World Computer,\" we will attempt, layer by layer, to recreate the engineering implementation of the world computer using examples from Ethereum and Polkadot, as before.\n\nLet's start with Ethereum. Ethereum began in 2015 with a state that can be characterized as a combination of the proof-of-work consensus algorithm, enabling the world computer to exist in a decentralized state (as discussed in Part 2). Additionally, the Ethereum Virtual Machine (EVM) was introduced, serving as a Turing-complete computational machine. Together, these two elements formed the first version of the world computer, sometimes referred to as a precursor. Within this context, decentralized applications, or smart contracts, began to emerge.\n\nOver the next 5 years, Ethereum lived a relatively unchanged life, undergoing some engineering tunings, such as a continuous increase in gas limits, with the exception of events like the Shanghai fork. Notably, during the second DEFCON held in Shanghai, a denial-of-service attack exploited a function in the virtual machine that consumed minimal gas but triggered significant computations on the Ethereum network. This led to memory overflow, effectively disrupting an entire Ethereum node. This incident highlights the intricate details that arise when dealing with a large and abstract solution like creating a virtual machine.\n\nMoving forward, a significant shift occurred around the end of the decade, particularly in 2020, with the advent of Ethereum 2.0. However, Ethereum 2.0 has now been deprecated, and I would characterize the real breakthrough as starting around 2019-2020. During this period, there was a true technological breakthrough in Ethereum, moving towards the concept of Ethereum 2.0. The moment of engineering change in Ethereum's architecture can be considered the event known as \"the merge,\" where the functionalities of the beacon chain were combined. The merge marked a significant shift in the paradigm of Ethereum, transitioning it into a slightly different state than what was on the board. The actual engineering change in Ethereum's architecture can be associated with \"the merge,\" where the functionalities of the beacon chain were integrated. For a detailed history of this, you can refer to the ethereum.org website, which provides an excellent article on the coexistence of the traditional Ethereum blockchain with the parallel blockchain launched in 2015 and the Ethereum Virtual Machine.\n\nWhen the merge occurred, we witnessed a new architectural representation, both at the network level and for individual nodes interacting with the Ethereum network. What was the actual change? For many, the merge signifies the transition from proof-of-work to proof-of-stake, which is indeed significant. It implies increased efficiency and fine-tuning, but it's still a tuning relative to one of the parameters. However, the more noteworthy internal engineering change for each network client was the split. There was no longer a single specific network client or a monolithic architecture. Instead, we got two components of a single node interacting with the Ethereum network.\n\nThe first part, which I labeled \"beacon chain\" on the diagram, essentially represents a collective image of all the innovations that came into the Ethereum client at the moment of the merge. The second part is the preserved virtual machine. Nevertheless, it's worth adding something here too. Dialogues truly began about replacing the virtual machine, which was exclusively tailored to work with smart contracts and smart contracts in a specific language—Solidity. This is because, by 2015, there were practically no interpreters left for smart contracts in languages other than Solidity, and the architecture appeared somewhat one-sided from the perspective of an Ethereum programmer. You learn a snippet of JavaScript in the form of Solidity, write smart contract code on it, and get your DApp, like Uniswap, for example.\n\nSince the emergence of a more complex Ethereum architecture, discussions have revolved around the idea that the virtual machine, which existed as a somewhat monolithic element from 2015, can also be replaced in the new architecture. The conversation shifted towards replacing it with something like WebAssembly (Wasm) or a more interesting solution from the perspective of writing code for the world computer. You could say, \"Wasm with a question mark.\"\n\nFrom the perspective of the Beacon Chain, it indeed operates on proof-of-stake, but what's more interesting is the inclusion of Gasper. This represents a modification of the original ideas about Casper. Casper, often referred to as the friendly ghost finality gadget, was introduced, perhaps even as early as Defcon 3 or 4, and maybe even discussed at Defcon 2—I don't recall precisely. But at the EthCC conference in Paris, which definitely took place in 2018, Vlad Zamfir and Vitalik, from different rooms, were discussing the emergence of Casper as a friendly ghost, overseeing participants in proof-of-stake and coming to the aid of the network when a node misbehaves. From this idea of Casper, Gasper emerges. Without delving into terminology too much, the consensus algorithm undergoes a shift, changing not only in terms of simplicity but also becoming more complex, similar to Polkadot. As I mentioned earlier, Polkadot has two consensus algorithms, Babe and Grandpa. Similarly, with Ethereum's Beacon Chain functionality, achieving consensus and finality is not as instantaneous. It involves epochs, and the network operates on a more complex scenario, reaching a state that is already somewhat dynamic, not frozen, and is essentially carved in stone.\n\n{What can be added in relation to 2024? For me, it was a prolonged observation and an attempt to understand whether Ethereum would eventually implement sharding or not. Sharding is the ability to exist not with a single blockchain but with multiple blockchains within one network. As I observed the merge and the simultaneous rise of Layer 2 (L2) networks, questions arose in my mind about whether sharding would indeed materialize. Sharding seemed interesting to me due to its homogeneity—having multiple chains that are almost identical, lacking any specific characteristics. It appeared to be an interesting approach, but not as flexible as a heterogeneous approach. In L2 networks, even several years ago, I could see the heterogeneity of Ethereum, its ability to work with various types of more specific blockchains. I was curious about the direction it would take—whether sharding, with its homogeneity, would displace L2 solutions or whether L2 solutions with a heterogeneous approach would saturate the Beacon Chain and the main nodes of the Ethereum network.\n\nToday, in 2024, based on articles on ethereum.org, it seems that sharding as a concept has been pushed back, and the focus is on helping various L2 networks integrate with the Beacon Chain and align with the main chain's functionality, which is now divided into two elements in the Ethereum network's architecture.\n\nTherefore, without delving into the details of how L2 networks are structured—although we'll touch upon that when we fill in the second part of the board—we should imagine that Ethereum is now a kind of Beacon Chain, a beacon, a guiding star for numerous L2 networks. These L2 networks can have more specific functionality, executing their logic according to a set of individual functions. This is somewhat in line with the idea of a Swiss Army knife—not making Ethereum a Swiss Army knife, but L2 networks are starting to differentiate in architecture. They duplicate the functionality of the abstract computing machine of Ethereum but perform it with lower gas costs or within their specific segment. Some are already thinking about tuning and making their L2 layer more efficient, focusing on specific functional capabilities. Thus, in my opinion, we are witnessing the emergence of heterogeneity in the world computer that aimed to be homogeneous. Also, it's essential not to forget that decentralized applications (dApps) still exist within the main blockchain, within that same blockchain that started in 2015. This means that during the merge, during the transition to the new architectural state, there was no wipeout, no erasure of the previous history. All decentralized applications and smart contracts underlying these applications continued to exist, and they continue to exist today, and probably tomorrow. This is a question that we will explore using Polkadot as an example, but there is still a feeling that it will be possible to settle a decentralized application in the Beacon Chain—dApps.\n\nIn summary, let's imagine the engineering implementation of today's Ethereum as a world computer. We have each network node consisting of two parts. The first layer is responsible for the Ethereum Virtual Machine (EVM), the actual functionality of the virtual machine or Turing complete machine, if we talk in theoretical terms. Perhaps we will see the emergence of alternatives to the virtual machine designed in 2015. These alternatives will likely surpass it in terms of more abstract programming possibilities than writing smart contracts in Solidity. Meanwhile, smart contracts in Solidity continue to feel comfortable. If you want to write functionality for the Ethereum main chain without creating any infrastructure on top of Ethereum, without offloading any calculations to make them cheaper, and so on, decentralized applications that you can write as smart contracts can still be housed in Ethereum's main blockchain. At the same time, Beacon Chain functionality has emerged, separating the consensus logic between validators from the main protocol of the computing machine. This allows for additional flexibility in how consensus should work and how it should be further modified without affecting the virtual machine itself. The example of Shanghai and Defcon 2, where a small opcode error caused a shutdown of part of the infrastructure, hints that it would be good to have such complex functionalities separated into two parts.\n\nWhat's interesting about the Beacon Chain? It is a more complex, comprehensive algorithm for achieving network synchronicity and finalization with the introduction of concepts such as \"epoch,\" and the presence of a ghost living within the network.\n\nLastly, what is important to consider now is that Ethereum is effectively putting an end to homogeneity, to the idea of getting a hundred identical blockchains working with the same virtual machine, where smart contracts written in Solidity can reside. Instead, various projects are proposing their own architectures or the same virtual machine taken beyond the main blockchain's limits. Alternatively, they are trying to build their more specific application, which, at the level of the Beacon Chain's main chain, is a smart contract written in Solidity. This is the current representation of Ethereum, which did not become Ethereum 2.0. It remains the same Ethereum—a project that once started with proof of work + Turing complete machine, transforming into this architecture.\n\nNow, let's take a look at how Polkadot emerged and evolved over the last 5 years. Polkadot came into existence five years after Ethereum, born out of the team that developed one of the best clients for Ethereum—Parity. Many might remember their web client, which, compared to Geth and other implementations, was probably much more pleasant to work with, at least from personal experience and the experience of colleagues.\n\nSecondly, Polkadot was, in my opinion, an extension of ideas that Gavin Wood wanted to incorporate into the development of Ethereum. Consequently, one might say that Ethereum, at some point, forked into two concepts.\nWhat did we have by the time Polkadot launched? The relay chain was launched. Interestingly, right? Beacon Chain & Relay Chain. What did the relay chain represent? Initially, there was no possibility to place a decentralized application there, write a smart contract for it, or upload your code in either WASM or Solidity. None of this was available at the time of the first block or the first few days of the Polkadot relay chain's existence. There was no way to add your runtime, which we'll talk about shortly, and it wasn't based on proof of stake; instead, it used proof of authority. This allowed certain nodes launched by Polkadot developers to survive the first months or weeks while attacks could be launched on the chain or if it behaved incorrectly. However, this was quickly changed, and the relay chain transitioned to proof of stake.\n\nIn the end, after a couple of months of the relay chain's existence without any decentralized application functionality, without the ability to connect your parachain or L2 network, without user capabilities, the network transitioned from an authority state to proof of stake. This gave developers the ability to upload their runtimes.\n\nAt this point, it's also interesting to discuss the differences between today's Ethereum and how the central part of Polkadot is structured. From the perspective of the heart, which we've already discussed, the picture will be absolutely the same not only for Ethereum and Polkadot but for any project that wants to be presented as an abstract computing machine. However, from an engineering and architectural standpoint, it's fascinating to observe Beacon Chain & Relay Chain. Here, we have a virtual machine, which has been inherited since 2015, but alternatives are being proposed. In the relay chain, there's the ability to upload your runtime. The runtime is, in fact, your virtual machine. For example, some parachains completely emulate the Ethereum Virtual Machine. It's written as a runtime, meaning you can essentially upload an Ethereum Virtual Machine analog to the parachain level in Polkadot or write more specific logic that works with four or five functions. Recall part one about the ideas— you can write your Swiss Army knife, but it won't require creating the entire infrastructure. You can implement specific functionality with certain functions at the runtime level, put it into the Polkadot relay chain, and the immutability of this runtime will be ensured by Polkadot validators.\n\nWhat happens next? Over the course of about a year, a layer of parachains begins to form around the relay chain. In terms of Ethereum implementation, you could say that L2 networks are quite similar to parachains. However, there's one interesting cross-network distinction that I find fascinating in Polkadot, and I'm trying to further understand how it will develop—namely, the second layer of validation and data availability checks. After a couple of years, Polkadot takes a shape like this. It's not just a relay chain where proof-of-stake validators protect the runtime of future parachains; an additional and crucial layer of data validation and availability checking emerges from parachains.\n\nAs you look at this diagram, try to notice the analogies that arise and the differences in engineering implementation details. So, what does this represent, and how does this scheme compare with Ethereum? We have an L2 project, in this case, with Polkadot, it's a parachain. A parachain also generates information blocks, which then go to the relay chain to be combined and release a relay chain block as the sum of all headers, headers, and more headers. The parachain collects transactions in a block using collators, which are not involved in validation. They don't stake anything in the relay chain; they only use the runtime, which is in the relay chain. They fetch it, apply it to transactions, perform necessary state transitions, form a block, and, crucially, provide proof of validity—a stamp containing cryptographic proofs that the collator correctly assembled the block. This information goes to the external validation ring of the relay chain. In this ring, there are internal validators of Polkadot—parachain collators. Again, they don't stake anything directly from the relay chain's point of view. Parachain implementations sometimes introduce their consensus among collators, and some don't. For example, in Robonomics, implementing a parachain, we find this paradigm more interesting, less burdensome, and it makes the network simpler while still remaining functionally substantial. Any collator, without reaching consensus with anyone—verified by us—can propose a block and some proof to the external layer. This is precisely why blocks are proposed, proofs of block assembly validity are offered, and there's an external ring. We don't need any consensus from parachain validators. Anyone can generate a block and send it, and if this node of the collator sends incorrect information to the parachain validators on the external ring, the validator at this level will reject it. It won't pass into the central part. But let's say the block was provided correctly by the collator. Our transactions got in; the collator calculated them, applying the runtime stored in the relay chain, executed all state transitions, gathered some proof of validity—validity of the assembled block—and passed it to the external ring of the relay chain. Here, every epoch, which is also part of the finalization, every epoch has validators from the relay chain diverging into parachains. Some of them stay in the center, and the others go to parachains. Their number ranges from 16 to 64 validators, and this figure, I believe, will change in the specification—somewhere more, somewhere less. However, parachain validators re-verify the information from one selected group of validators about everything coming from the collator being correct, that work has been done in accordance with the runtime, and that the proof of validity is indeed valid. The selected segment of relay chain validators who already have something staked respond, or rather, chirp among themselves. They respond to the chosen main block producer of the parachain, so to speak, saying, \"Yes, we agree. There are no problems. You can carry it through the entire external ring inside.\"\n\nAnd thus, almost all information formed on the parachain collators, with verification on the external ring, enters the internal one. The lower part, not that it's physically at the bottom, still constitutes the external ring—data availability. Data starts to be checked at this stage, meaning that on the external ring, not only the correctness of block assembly is verified, but the process of preparing for distribution within the Polkadot network begins, ensuring that the block information will not be lost in the future. Here, precisely, is what I mentioned in the second part about chunks, like CD RW. At this stage of block preparation for transfer to the internal ring, the data availability layer is formed as a service, something that is currently also attempted by some projects in Ethereum. Some projects put additional redundant information directly into smart contracts, necessary for checking what is happening on the L2 layer and, if necessary, slashing or punishing those who did it incorrectly. It's impossible to overcome the external ring without distributing block information and without rechecking dozens of nodes with stakes laid down on the assumption that the runtime must work correctly.\n\nThus, information that has passed through the external ring is already quite trustworthy, probably yes, you can say that, and on the internal ring, work is mainly done not with parachain blocks, but their block headers are collected into one big header. That is, from many headers, one header of a relay chain block is assembled—a mechanism of linking in Shared Security, as mentioned in Polkadot, which ensures the security of parachains. One could say that parachains are validated and reach a state where the service exists in a distributed decentralized form on the external ring. In the internal ring, the information that has entered attempts to come together in one hyperblock, which should precisely link everything together. There are no calculations happening there; there is no recalculation of absolutely everything. The assembly of the final block takes place, so to speak, in the current iteration of the world computer, to put a point on the question of whether the transaction has passed in a particular parachain. We must assemble a hyperblock that contains not all the information from the parachains but gathers all the headers verified on the external ring of parachains into one large block. And thus, our world computer in Polkadot operates.\n\nLet's take another look at these two schemes together: relay chain, beacon chain, runtime, secured by proof of stake, where someone stakes their funds to validate that they will always perform their work correctly. There's a virtual machine where you can also stake your funds, and if you perform any computation or state transition not in accordance with the Ethereum Virtual Machine's specification, you'll be penalized.\n\nIn Polkadot, there's an additional external layer, which seems to be one of the main advantages, such pleasant perks of the engineering implementation that, in my opinion, should be present here. It should appear between L2 networks and the beacon chain, which exists in Ethereum. By the way, some say that the term \"beacon chain\" is dying out again and is misunderstood, but I really like to use it in analogy with the \"relay chain,\" a term from Ethereum's roadmap.\n\nIn Polkadot, there's an external layer that allows, or rather, I think it was invented to solve many issues arising when you have L2 or a set of blockchains that need to be connected. On this layer, an engineering mechanism for the distribution of information is implemented to make it available in a decentralized network. Additional algorithms are introduced for checking not only validity but also the availability of information by validators. Plus, there's a mechanism for randomly assigning a portion of Polkadot validators to specific parachains every epoch. So, not the same validators service parachains every epoch; they are shuffled and sent to different parachains each epoch. When transferring a block from the external ring to the internal one, validators are rechecked along the way and coordinated with those assigned to the parachain. Currently, this process doesn't exist, but I think it will appear at some point.\nAnd perhaps the last point is about collators, which are implemented quite interestingly in parachains today. They can be consensus or exist without consensus, but, in fact, it works. As for questions in L2 networks with decentralized sequencers or how blocks will be generated and verified before settling in the virtual machine - these are separate questions for the implementation of Ethereum in a heterogeneous format. On this day, it is quite well implemented in Polkadot, in my opinion. However, this doesn't mean that Polkadot is ahead of the entire planet, and it will never catch up with Ethereum. Although it is this architecture that attracts me to continue working and hoping that Polkadot will continue to develop well in terms of technologies because I haven't seen anything like this in all the connected aspects.\n\nAnd perhaps one more interesting story in this part of the lecture: so far, we can hardly imagine proper cross-chain messages between L2 networks in Ethereum. Maybe I missed something in the papers, but when you don't have an external ring and issues like collators, paravalidators, and data availability services are not resolved, thinking about how two L2 layers can communicate is challenging. Yet, in Polkadot, it exists. Even horizontally, through the relay chain, meaning directly, one can send a transaction securely from one parachain to another, without trusting any bridges between these two parachains. This is another crucial functionality that will likely need to be implemented at the level of connecting L2 networks. Smart contracts in Ethereum communicate well. We have created many chains of linked smart contracts, where one triggers another. With this, there is no problem. But when we say that almost all applications are moving to the L2 layer in a heterogeneous network, I hear that if you live in a specific area, you won't be able to get out. That's not the case at the level of parachains and implementation in Polkadot. Both architectures are worth watching, as, in my opinion, the engineering implementation follows the mainstream path of becoming a global computer. They differ slightly, but there are many similarities. There's an enormous amount of engineering work everywhere. As we see, human civilization, in the form of a multitude of researchers, engineers, and growing developers with significant resources for further development, is moving roughly in the same direction from the smallest early stage to probably some future establishment of the world's computer, all on the same tracks.\n","fileInfo":{"path":"en/learn/world-computer/emergence-of-the-world-computer.md","name":"emergence-of-the-world-computer"},"defaultName":"World computer in your home","lastUpdate":null}},"context":{}}